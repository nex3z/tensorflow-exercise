{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'aclImdb/train'\n",
    "VAL_SPLIT = 0.2\n",
    "# BATCH_SIZE = 1024\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "OOV_TOKEN = '<OOV>'\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "TRUNCATING = 'post'\n",
    "PADDING = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=1, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=1, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='validation', \n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "for sentence, label in train_ds:\n",
    "    train_sentences.append(sentence[0].numpy().decode('utf8'))\n",
    "    train_labels.append(label[0].numpy())\n",
    "\n",
    "for sentence, label in val_ds:\n",
    "    val_sentences.append(sentence[0].numpy().decode('utf8'))\n",
    "    val_labels.append(label[0].numpy())\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<OOV>', 'the', 'and', 'a', 'of', 'to', 'is', 'br', 'in', 'it']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[reverse_word_index[i] for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_sequences) = <class 'list'>\n",
      "train_sequences[0] = [2, 324, 1, 4, 53, 350, 20, 17, 4, 174, 5, 1612, 224, 431, 3, 894, 2, 529, 1772, 51, 2, 1341, 45, 1065, 4, 350, 20, 5, 316, 485, 13, 553, 27, 1054, 6, 98, 82, 350, 20, 13, 11, 26, 108, 11, 26, 1135, 12, 20, 31, 2, 3844, 623, 686, 11, 79, 380, 13, 10, 7, 601, 6, 1087, 1, 3, 960, 80, 9, 98, 6917, 49, 264, 44, 5, 10, 7, 2, 189, 13, 80, 17, 1008, 24, 114, 7812, 6, 65, 541, 7, 114, 557, 10, 7, 4, 2030, 6, 27, 2108, 32, 98, 1087, 13, 488, 6, 137, 930, 11, 435, 13, 2, 1065, 78, 2157, 4, 332, 173, 6, 12, 1, 11, 166, 930, 6, 843, 2, 730, 277, 170, 1, 6, 1, 1753, 9, 3845, 4, 20, 5, 7675, 3, 485, 17, 4, 2030, 9, 328]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "print(f\"type(train_sequences) = {type(train_sequences)}\")\n",
    "print(f\"train_sequences[0] = {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_padded) = <class 'numpy.ndarray'>\n",
      "train_padded.shape = (20000, 100)\n",
      "train_padded[0] = \n",
      "[   2  324    1    4   53  350   20   17    4  174    5 1612  224  431\n",
      "    3  894    2  529 1772   51    2 1341   45 1065    4  350   20    5\n",
      "  316  485   13  553   27 1054    6   98   82  350   20   13   11   26\n",
      "  108   11   26 1135   12   20   31    2 3844  623  686   11   79  380\n",
      "   13   10    7  601    6 1087    1    3  960   80    9   98 6917   49\n",
      "  264   44    5   10    7    2  189   13   80   17 1008   24  114 7812\n",
      "    6   65  541    7  114  557   10    7    4 2030    6   27 2108   32\n",
      "   98 1087]\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=PADDING, truncating=TRUNCATING)\n",
    "print(f\"type(train_padded) = {type(train_padded)}\")\n",
    "print(f\"train_padded.shape = {train_padded.shape}\")\n",
    "print(f\"train_padded[0] = \\n{train_padded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: empty expression not allowed (<ipython-input-11-ace1c8d0e59d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-ace1c8d0e59d>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(f\"{'-' * 16} original {'-' * 16} \\n{}\")\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m f-string: empty expression not allowed\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'-' * 16} decoded {'-' * 16} \\n{decode_sequence(train_padded[0])}\")\n",
    "print(f\"{'-' * 16} original {'-' * 16} \\n{}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
