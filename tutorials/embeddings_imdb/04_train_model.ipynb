{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'aclImdb/train'\n",
    "VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 1024\n",
    "SEED = 42\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "EMBEDDING_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ").cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='validation', \n",
    "    seed=SEED\n",
    ").cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html, f'[{re.escape(string.punctuation)}]', '')\n",
    "\n",
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, name='embedding'),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.4912WARNING:tensorflow:From d:\\dev\\python_env\\py38_tf23\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 0.6923 - accuracy: 0.5019 - val_loss: 0.6908 - val_accuracy: 0.4924\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.6882 - accuracy: 0.5019 - val_loss: 0.6855 - val_accuracy: 0.4924\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.6808 - accuracy: 0.5019 - val_loss: 0.6770 - val_accuracy: 0.4924\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 2s 117ms/step - loss: 0.6690 - accuracy: 0.5019 - val_loss: 0.6640 - val_accuracy: 0.4924\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.6517 - accuracy: 0.5019 - val_loss: 0.6457 - val_accuracy: 0.4924\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 2s 116ms/step - loss: 0.6281 - accuracy: 0.5110 - val_loss: 0.6222 - val_accuracy: 0.5174\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.5991 - accuracy: 0.5816 - val_loss: 0.5950 - val_accuracy: 0.5820\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.5664 - accuracy: 0.6610 - val_loss: 0.5658 - val_accuracy: 0.6400\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 2s 113ms/step - loss: 0.5321 - accuracy: 0.7166 - val_loss: 0.5364 - val_accuracy: 0.6858\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.4974 - accuracy: 0.7560 - val_loss: 0.5083 - val_accuracy: 0.7180\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.4648 - accuracy: 0.7840 - val_loss: 0.4834 - val_accuracy: 0.7388\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 2s 120ms/step - loss: 0.4355 - accuracy: 0.8036 - val_loss: 0.4624 - val_accuracy: 0.7558\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 118ms/step - loss: 0.4099 - accuracy: 0.8188 - val_loss: 0.4450 - val_accuracy: 0.7682\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 119ms/step - loss: 0.3876 - accuracy: 0.8310 - val_loss: 0.4309 - val_accuracy: 0.7778\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.3681 - accuracy: 0.8418 - val_loss: 0.4195 - val_accuracy: 0.7856\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds, \n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0: continue # skip 0, it's padding.\n",
    "    vec = weights[index] \n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
