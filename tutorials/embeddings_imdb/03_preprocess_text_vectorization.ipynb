{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'aclImdb/train'\n",
    "VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 1024\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='training',\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN_DIR, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_split=VAL_SPLIT, \n",
    "    subset='validation', \n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"I married a Japanese woman 14 years ago. We're still together.<br /><br />However in the 1950's it would never have been as easy.<br /><br />Life in the military had been mined for action, drama, and comedy for years by this point. Mined to death. The mixed relationships gave it new ground to cover. This is old hat today, but then...? Marrying an Asian back then meant you either owed somebody something or you were a freak of some sort. This touched on both possibilities along with the third. Maybe it IS love? <br /><br />Brando did his usual good job. Garner did a better job than he usually does. He's good, but this showed how good he could be. Umecki-chan had a helluva debut here and while I think she earned her statue, she didn't really stretch. It was a role that no one who hadn't been overseas would have recognized and the newness was the corker.<br /><br />The real scene stealer was Red Buttons. Red was the best thing in this film. Bank on it. And the Japanese lifestyles were shown in an admirable light as well.<br /><br />A classic.\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in train_ds.take(1):\n",
    "    break\n",
    "\n",
    "print(x_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b\"Reese Witherspoon plays Dani, a young country girl that falls madly in love with the new 17 year old neighbor, Court, played by Jason London. Court tries his best to make Dani realize that the difference in their ages would make a love relationship improbable. Soon the nubile charm of Dani starts winning over Court's will. Next enters the meeting of Dani's older sister, played by Emily Warfield, and the beginning of a short lived love/jealousy problem.<br /><br />Tess Harper and Sam Waterston round out the cast. This is a fresh, free spirited; but heartbreaking drama that touches down deep. Feel free to cry.\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html, f'[{re.escape(string.punctuation)}]', '')\n",
    "\n",
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
       "array([[5128,  182,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(['hello world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocab) = 10000\n",
      "vocab[:10] = ['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(f\"len(vocab) = {len(vocab)}\")\n",
    "print(f\"vocab[:10] = {vocab[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([vocab[i] for i in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch_processed.shape = (1024, 100)\n"
     ]
    }
   ],
   "source": [
    "x_batch_processed = vectorize_layer(x_batch)\n",
    "print(f\"x_batch_processed.shape = {x_batch_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i married a japanese woman 14 years ago were still together however in the 1950s it would never have been as easy life in the military had been [UNK] for action drama and comedy for years by this point [UNK] to death the mixed relationships gave it new ground to cover this is old hat today but then marrying an asian back then meant you either [UNK] somebody something or you were a freak of some sort this touched on both possibilities along with the third maybe it is love brando did his usual good job garner did a better'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(x_batch_processed[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
