{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "OOV_TOKEN = '<OOV>'\n",
    "MAX_LENGTH = 120\n",
    "TRUNCATING = 'post'\n",
    "PADDING = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_ds, test_ds = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "\n",
    "val_sentences = []\n",
    "val_labels = []\n",
    "\n",
    "for sentence, label in train_ds:\n",
    "    train_sentences.append(sentence.numpy().decode('utf8'))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "for sentence, label in test_ds:\n",
    "    val_sentences.append(sentence.numpy().decode('utf8'))\n",
    "    val_labels.append(label.numpy())\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_sequences) = <class 'list'>\n",
      "train_sequences[0] = [12, 14, 33, 425, 392, 18, 90, 28, 1, 9, 32, 1366, 3585, 40, 486, 1, 197, 24, 85, 154, 19, 12, 213, 329, 28, 66, 247, 215, 9, 477, 58, 66, 85, 114, 98, 22, 5675, 12, 1322, 643, 767, 12, 18, 7, 33, 400, 8170, 176, 2455, 416, 2, 89, 1231, 137, 69, 146, 52, 2, 1, 7577, 69, 229, 66, 2933, 16, 1, 2904, 1, 1, 1479, 4940, 3, 39, 3900, 117, 1584, 17, 3585, 14, 162, 19, 4, 1231, 917, 7917, 9, 4, 18, 13, 14, 4139, 5, 99, 145, 1214, 11, 242, 683, 13, 48, 24, 100, 38, 12, 7181, 5515, 38, 1366, 1, 50, 401, 11, 98, 1197, 867, 141, 10]\n"
     ]
    }
   ],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "print(f\"type(train_sequences) = {type(train_sequences)}\")\n",
    "print(f\"train_sequences[0] = {train_sequences[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(train_padded) = <class 'numpy.ndarray'>\n",
      "train_padded.shape = (25000, 120)\n",
      "train_padded[0] = \n",
      "[  12   14   33  425  392   18   90   28    1    9   32 1366 3585   40\n",
      "  486    1  197   24   85  154   19   12  213  329   28   66  247  215\n",
      "    9  477   58   66   85  114   98   22 5675   12 1322  643  767   12\n",
      "   18    7   33  400 8170  176 2455  416    2   89 1231  137   69  146\n",
      "   52    2    1 7577   69  229   66 2933   16    1 2904    1    1 1479\n",
      " 4940    3   39 3900  117 1584   17 3585   14  162   19    4 1231  917\n",
      " 7917    9    4   18   13   14 4139    5   99  145 1214   11  242  683\n",
      "   13   48   24  100   38   12 7181 5515   38 1366    1   50  401   11\n",
      "   98 1197  867  141   10    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding=PADDING, truncating=TRUNCATING)\n",
    "print(f\"type(train_padded) = {type(train_padded)}\")\n",
    "print(f\"train_padded.shape = {train_padded.shape}\")\n",
    "print(f\"train_padded[0] = \\n{train_padded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- decoded ---------------- \n",
      "this was an absolutely terrible movie don't be <OOV> in by christopher walken or michael <OOV> both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the <OOV> rebels were making their cases for <OOV> maria <OOV> <OOV> appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor's like christopher <OOV> good name i could barely sit through it ? ? ?\n",
      "---------------- original ---------------- \n",
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'-' * 16} decoded {'-' * 16} \\n{decode_sequence(train_padded[0])}\")\n",
    "print(f\"{'-' * 16} original {'-' * 16} \\n{train_sentences[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
