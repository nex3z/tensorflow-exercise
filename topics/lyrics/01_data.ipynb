{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = keras.utils.get_file(\n",
    "    os.path.basename(DATA_URL),\n",
    "    DATA_URL,\n",
    "    cache_dir='./',\n",
    "    cache_subdir=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(corpus) = 1693\n"
     ]
    }
   ],
   "source": [
    "data = open(data_path).read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "print(f\"len(corpus) = {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['come all ye maidens young and fair', 'and you that are blooming in your prime', 'always beware and keep your garden fair', 'let no man steal away your thyme', 'for thyme it is a precious thing']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 items in word_index = [('the', 1), ('and', 2), ('i', 3), ('to', 4), ('a', 5)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "print(f\"first 5 items in word_index = {list(word_index.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(sequence):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_sequence_len = 16\n",
      "input_sequences.shape = (12038, 16)\n"
     ]
    }
   ],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    sequence = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(sequence)):\n",
    "        n_gram_sequence = sequence[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "        \n",
    "max_sequence_len = max(map(len, input_sequences))\n",
    "print(f\"max_sequence_len = {max_sequence_len}\")\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "input_sequences = np.array(input_sequences)\n",
    "print(f\"input_sequences.shape = {input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs.shape = (12038, 15), labels.shape = (12038,), ys.shape = (12038, 2690)\n"
     ]
    }
   ],
   "source": [
    "xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "ys = keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "print(f\"xs.shape = {xs.shape}, labels.shape = {labels.shape}, ys.shape = {ys.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0   51   12\n",
      "   96 1217]\n",
      "? ? ? ? ? ? ? ? ? ? ? ? come all ye maidens\n"
     ]
    }
   ],
   "source": [
    "print(input_sequences[2])\n",
    "print(decode_sequence(input_sequences[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0  0  0  0  0  0  0 51 12 96]\n",
      "? ? ? ? ? ? ? ? ? ? ? ? come all ye\n"
     ]
    }
   ],
   "source": [
    "print(xs[2])\n",
    "print(decode_sequence(xs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(ys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217\n",
      "maidens\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(ys[2]))\n",
    "print(decode_sequence([np.argmax(ys[2])]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
